{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cafe sales analysis\n",
    "By Alan Tolubayev\n",
    "\n",
    "[Cafe Sales](https://www.kaggle.com/datasets/ahmedmohamed2003/cafe-sales-dirty-data-for-cleaning-training) dataset was used for analysis.\n",
    "\n",
    "#### Business Context\n",
    "\n",
    "As a data science consultant specializing in retail analytics for small to medium-sized businesses, I was approached by a Thailand-based cafe to analyze their sales for the past year and help them to boost revenue.\n",
    "\n",
    "#### Objective\n",
    "The cafe has collected transactional data, however, they lack the expertise to extract meaningful insights from it. As a data scientist, my role is to help with insights.\n",
    "1. What are the limitations of the data they provided?\n",
    "2. What are the top selling items?\n",
    "3. How do sales behave month over month?\n",
    "4. What strategies can maximize profitability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Here\n",
    "The code below will check for missing libraries and install them. Make sure the requirements file is located in the same folder as the analysis file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "def install_required_packages():\n",
    "    required_packages = [\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"matplotlib\",\n",
    "        \"seaborn\",\n",
    "    ]\n",
    "    \n",
    "    missing_packages = []\n",
    "    for package in required_packages:\n",
    "        pkg_name = package.split('==')[0]  # in case you add versions later\n",
    "        if importlib.util.find_spec(pkg_name) is None:\n",
    "            missing_packages.append(package)\n",
    "\n",
    "    if missing_packages:\n",
    "        print(f\"ðŸ” Installing missing packages: {missing_packages}\")\n",
    "        for pkg in missing_packages:\n",
    "            if os.system(f\"pip install {pkg}\") != 0:\n",
    "                print(f\"âŒ Failed to install {pkg}\")\n",
    "        print(\"âœ… Installation complete.\")\n",
    "    else:\n",
    "        print(\"âœ… All required packages are already installed.\\nYou're good to go!\")\n",
    "\n",
    "# Run the installer\n",
    "install_required_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "def load_data(file, delimiters=[',', ';', '\\t', '|']):\n",
    "    for sep in delimiters:\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep=sep, encoding='utf-8')\n",
    "            if not df.empty:\n",
    "                print(f\"âœ… Loaded data using '{sep}' delimiter.\\nPreview:\")\n",
    "                print(df.head())\n",
    "                return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    print(\"âŒ Failed to load data. Check the file and delimiters.\")\n",
    "    return None\n",
    "raw_df = load_data('cafe_sales.csv')\n",
    "\n",
    "cleaning_df = raw_df.copy()\n",
    "\n",
    "print(f\"Duplicate rows: {cleaning_df.duplicated().sum()}\")\n",
    "print(f'\\nMissing values count:\\n{cleaning_df.isnull().sum()}')  # Count missing values per column\n",
    "print(f'\\nMissing values sample:\\n{cleaning_df[cleaning_df.isnull().any(axis=1)]}')  # Show rows with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. **Transaction ID** is the primary key in our exercise. We have 10,000 unique transactions. We will use this number as the baseline for other columns.\n",
    "2. Item represents sold items in the order. We have 9,667 values. That means we are missing 333 values.\n",
    "3. Quantity column is missing 138 values.\n",
    "4. **Price per unit** is misssing 179 values. This column represents a monetary value of an item sold. I will review the Items column to find the prices, create a dictionary with \"Item\" : \"Price per unit\" values and replace the missing values.\n",
    "5. **Total Spent** is a calculated column. It supposed to be a multiplication of quantity and price per unit. I will use this logic to recalculate and review the column.\n",
    "5. Payment methods also missing 2,579 values (~26%). This column is one of the highest missing value column. We will investigate it futher.\n",
    "6. Location. 3,265 missing values (~33%). We will also investigate this column futher.\n",
    "7. Transaction date. Missing 159 values.\n",
    "\n",
    "**Next steps:**\n",
    "1. Changing data types to make sure the items are strigns and prices represent float values.\n",
    "1. Replacing \"UNKNOWN\" values with in transaction dates and checking why it happens with a client.\n",
    "1. Impude missing values to avoid creating a biased dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_df = cleaning_df.replace(['ERROR', 'UNKNOWN', np.nan], '') # Replace ERROR values with empty strings to clean later\n",
    "\n",
    "columns_to_int = ['Quantity']\n",
    "columns_to_float = ['Price Per Unit', 'Total Spent']\n",
    "\n",
    "for col in columns_to_int:\n",
    "    cleaning_df[col] = pd.to_numeric(cleaning_df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "for col in columns_to_float:\n",
    "    cleaning_df[col] = pd.to_numeric(cleaning_df[col], errors='coerce').fillna(0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to check and impude missing data in \"Item\", \"Payment Method\", \"Location\", and \"Transaction Date\" columns. In the original dataset we saw some Errors and missing values. Since we converted all Errors and Unknown values to empty strings, after conversion they became 'NaT' datatypes which means 'Not a Time'. To find them we will use [isna](https://pandas.pydata.org/docs/reference/api/pandas.isna.html) function.\n",
    "\n",
    "Dates are important indicators for pattern analysis. In order to analyze the data futher we will use Python's mode function to find the most frequent date and replace the NaT values with it. We will use Panda's [pd.to_datetime](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) and [dt.normalize](https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.normalize.html) function to fix the missing transaction dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = {\n",
    "    'Item': '',\n",
    "    'Payment Method': '',\n",
    "    'Location': '',\n",
    "    'Transaction Date': pd.NaT  # Use NaT for dates\n",
    "}\n",
    "\n",
    "total_values = cleaning_df['Transaction ID'].count()\n",
    "print(f\"Total number of values: {total_values}\")\n",
    "\n",
    "for col, missing_value in columns_to_check.items():\n",
    "    if missing_value == '':\n",
    "        missing_count = cleaning_df[cleaning_df[col] == '']['Transaction ID'].count()\n",
    "    else:\n",
    "        missing_count = cleaning_df[cleaning_df[col].isna()]['Transaction ID'].count()\n",
    "\n",
    "    missing_ratio = missing_count / total_values * 100\n",
    "    print(f\"Number of missing {col.lower()}: {missing_count}\")\n",
    "    print(f\"Missing {col.lower()} ratio: {missing_ratio:.2f}%\")\n",
    "\n",
    "def clean_column(df, column_name):\n",
    "    df[column_name] = df[column_name].replace('', 'Other')\n",
    "    replaced_count = df[df[column_name] == 'Other']['Transaction ID'].count()\n",
    "    print(f'{column_name} replaced with \"Other\": {replaced_count}')\n",
    "    return df\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_transaction_date(df):\n",
    "    df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce').dt.normalize()\n",
    "\n",
    "    df = df[df['Transaction Date'].notna()]\n",
    "\n",
    "    print(f\"Number of missing dates after cleaning: {df['Transaction Date'].isna().sum()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "columns_to_clean = ['Item', 'Payment Method', 'Location']\n",
    "for column in columns_to_clean:\n",
    "    cleaning_df = clean_column(cleaning_df, column)\n",
    "\n",
    "cleaning_df = clean_transaction_date(cleaning_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for consistency in numerical data:\n",
    "- There are items where Price Per Unit is zero\n",
    "- In some cases Quantity is zero\n",
    "- Total spent is zero\n",
    "\n",
    "To fix this issue we will create a dictionary of iteams and their prices and make sure all the itesm have a price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppu = cleaning_df[cleaning_df['Price Per Unit'] == 0]['Transaction ID'].count()\n",
    "qty = cleaning_df[cleaning_df['Quantity'] == 0]['Transaction ID'].count()\n",
    "total = cleaning_df[cleaning_df['Total Spent'] == 0]['Transaction ID'].count()\n",
    "print(f'Missing prices per unit: {ppu}')\n",
    "print(f'Missing quantity: {qty}')\n",
    "print(f'Missing totals spent: {total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix the inconsistency in the transactional data we will create a dictionary with prices for items.\n",
    "And implement a function that will check for Total Spent is above zero.\n",
    "If the total spent doesn't equal to Price Per Item multiplied by Quantity:\n",
    "1. We look for columns where Price per Unit or Quantity are zero and replace the Price Per Unit from the dictionary\n",
    "2. We look for Total Spent and Price per Item to recalculate Quantity column by dividing Total Spent by Price Per Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_price_dict = (\n",
    "    cleaning_df[cleaning_df['Price Per Unit'] > 0]  # Keep only prices > 0\n",
    "    [['Item', 'Price Per Unit']]\n",
    "    .drop_duplicates()\n",
    "    .set_index('Item')['Price Per Unit']\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"Item-Price Dictionary:\")\n",
    "print(item_price_dict)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def fix_inconsistent_transactions(df, price_dict):\n",
    "    df = df.copy()\n",
    "\n",
    "    def fix_row(row):\n",
    "        item = row['Item']\n",
    "        price = row['Price Per Unit'] or price_dict.get(item, 0)\n",
    "        qty = row['Quantity']\n",
    "        total = row['Total Spent']\n",
    "\n",
    "        if price == 0:\n",
    "            price = price_dict.get(item, price)\n",
    "\n",
    "        if qty == 0:\n",
    "            qty = total / price if total != 0 and price != 0 else 1\n",
    "\n",
    "        if total == 0 or total != qty * price:\n",
    "            total = qty * price\n",
    "\n",
    "        row['Price Per Unit'], row['Quantity'], row['Total Spent'] = price, qty, total\n",
    "        return row\n",
    "\n",
    "    return df.apply(fix_row, axis=1)\n",
    "\n",
    "cleaning_df = fix_inconsistent_transactions(cleaning_df, item_price_dict)\n",
    "\n",
    "inconsistent_rows = cleaning_df[ \n",
    "    (cleaning_df['Total Spent'] != cleaning_df['Quantity'] * cleaning_df['Price Per Unit'])\n",
    "]\n",
    "print(f\"Remaining inconsistent transactions: {inconsistent_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_methods = cleaning_df['Payment Method'].unique()\n",
    "locations = cleaning_df['Location'].unique()\n",
    "duplicates = cleaning_df.duplicated().sum()\n",
    "\n",
    "pay_methods_str = \", \".join(pay_methods)\n",
    "locations_str = \", \".join(locations)\n",
    "\n",
    "print(f\"Unique payment methods: {pay_methods_str}\")\n",
    "print(f\"Unique locations: {locations_str}\")\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "inconsistent_rows = cleaning_df[ \n",
    "    cleaning_df['Total Spent'] != cleaning_df['Quantity'] * cleaning_df['Price Per Unit']\n",
    "]\n",
    "\n",
    "if inconsistent_rows.empty:\n",
    "    print(\"Inconsistent data: None found\")\n",
    "else:\n",
    "    print(\"Inconsistent data:\")\n",
    "    print(inconsistent_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: What are the limitations of the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite my best effort, there are still limitations to the dataset that we should be aware of:\n",
    "1. Missing and Imputed data. 30% of payment data and 37.8% of location data were imputed during cleaning. Unfortunately we won't be able to use it for analysis.\n",
    "2. Missing client data. Without client data I won't be able to track loyalty, repeat purchases, or provide a segmentation analysis.\n",
    "3. Low depth of data. Without margins and expenses I won't be able to make a Profit and Loss analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = cleaning_df.copy()\n",
    "print(f\"Replaced payment method data: {clean_df[clean_df['Payment Method'] == 'Other']['Transaction ID'].count()}\")\n",
    "print(f\"Replaced location data: {clean_df[clean_df['Location'] == 'Other']['Transaction ID'].count()}\")\n",
    "print(f\"Replaced Item data: {clean_df[clean_df['Item'] == 'Other']['Transaction ID'].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: What are the top selling items?\n",
    "\n",
    "Identifiying top selling items would help to prioritize best performers and create bundles or combos to increace Average Order Value.\n",
    "Calculating top selling items is done by grouping items by quantity and order value. To prevent bias towards the quantity of items, I'm adding a sum of total spent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_selling_items = clean_df.groupby('Item')[['Quantity', 'Total Spent']].sum().sort_values(by='Quantity', ascending=False)\n",
    "\n",
    "# Plotting top 10 items by Quantity\n",
    "top_5_items = top_selling_items.head(5)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "bar_plot = sns.barplot(data=top_5_items, x='Item', y='Quantity', hue='Total Spent', legend=False, palette='viridis')\n",
    "\n",
    "# Annotate each bar with Total Spent values\n",
    "for index, row in top_5_items.iterrows():\n",
    "    bar_plot.text(index, row['Quantity'], f\"{row['Total Spent']}\", color='white', ha=\"center\", va=\"bottom\", fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.title(\"Top 5 selling items by quantity\", fontsize=12)\n",
    "plt.xlabel(\"\", fontsize=10)\n",
    "plt.ylabel(\"Total quantity sold\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coffee and juice are the best performing items on the menu, but we can also see food items being right after. I'd recommend creating a coffee + sandwich combo or salad + juice combo to increase the Average Order Value and sales performance during the day. Unfortunately we don't have time data during the day to check when people are more likely to buy these items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: How do sales behave month over month?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales by month\n",
    "Helps to identify seasonal trends and busy periods to optimize stocks and plan promotions around peak months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['Month'] = clean_df['Transaction Date'].dt.month_name()\n",
    "\n",
    "monthly_sales = clean_df.groupby('Month')['Total Spent'].sum()\n",
    "\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "monthly_sales = monthly_sales.reindex(month_order).dropna().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "bar_plot = sns.barplot(data=monthly_sales, x='Month', y='Total Spent', palette='crest', hue='Total Spent', legend=False)\n",
    "\n",
    "for idx, row in monthly_sales.iterrows():\n",
    "    bar_plot.text(\n",
    "        idx, \n",
    "        row['Total Spent'], \n",
    "        f\"${row['Total Spent']:.2f}\", \n",
    "        color='white', \n",
    "        ha=\"center\", \n",
    "        va=\"bottom\", \n",
    "        fontsize=8, \n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.title(\"Sales by Month\", fontsize=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Revenue\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis identified February, July, September, and November as months with notably lower sales performance compared to other months. To stabilize revenue, we recommend launching targeted promotional campaigns during these months.\n",
    "\n",
    "**Potential promotional strategies include:**\t\n",
    "- Limited offera to encourage more frequent visits or increase spending per visit.\n",
    "- Product combos with attractive prices to boost sales.\n",
    "- Introduce rewards that customers can earn in these lower-performing periods.\n",
    "- Utilize social media to highlight promotions specifically for these months.\n",
    "\n",
    "Implementing these promotional tactics should drive increased traffic and sales during traditionally slow periods, improving cafeâ€™s overall revenue consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Order Value over time\n",
    "Heps to identify months will lower AOV and plan promotions around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clean_df['Month'] = clean_df['Transaction Date'].dt.month_name()\n",
    "\n",
    "monthly_aov = clean_df.groupby('Month')['Total Spent'].mean()\n",
    "\n",
    "month_order = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "               'July', 'August', 'September', 'October', 'November', 'December']\n",
    "monthly_aov = monthly_aov.reindex(month_order).dropna()\n",
    "\n",
    "# Plot Monthly AOV\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x=monthly_aov.index, y=monthly_aov.values)\n",
    "\n",
    "# Annotate bars clearly with values\n",
    "for idx, value in enumerate(monthly_aov):\n",
    "    plt.text(idx, value, f\"{value:.2f}\", ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Titles and labels\n",
    "plt.title('Average Order Value month-over-month', fontsize=16)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('\\nAverage Order Value\\n', fontsize=12)\n",
    "plt.xticks()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: What strategies can maximize profitability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize profitability I suggest the cafe to: \n",
    "1. Optimize offerings, like coffee and juice.\n",
    "2. Create combos (coffee + sandwich) to increase the Average Order Value.\n",
    "3. Start promo-campaigns around the low performing months like February, July, and September\n",
    "4. Consider investing into a loyalty program that will increase the Average Order Value and attract new customers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
